{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60ca457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb  2 11:19:26 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   77C    P0             37W /   70W |   13740MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0470107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e03dd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eed09b84e6148e387ecafce75170666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q -U transformers accelerate bitsandbytes\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# 1. Tải Tokenizer và Model với cấu hình 4-bit để không bị tràn RAM T4\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    quantization_config={\"load_in_4bit\": True}, # Nén để chạy vừa GPU 16GB\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. Tạo pipeline chat\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f2a094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Làm quen với lỗ đen vũ trụ là một trải nghiệm thú vị, đặc biệt là khi chúng ta làm đơn giản và hấp dẫn như có thể:\n",
      "\n",
      "Lỗ đen là một khu vực rất đặc biệt trong vũ trụ. Hãy tưởng tượng bạn đang ở trung tâm của một cái vòng xoáy khổng lồ - một lỗ xoáy lớn đến nỗi không gì có thể thoát ra khỏi nó. Lỗ đen là một điểm trong không gian nơi mà lực hấp dẫn trở nên mạnh mẽ đến mức không có gì - kể cả ánh sáng - có thể thoát ra khỏi nó.\n",
      "\n",
      "Đôi khi, các ngôi sao lớn hoặc hành tinh lớn bị kéo vào lỗ đen. Điều này giống như khi bạn thả một quả bóng vào nước - quả bóng sẽ chìm xuống đáy và không bao giờ trở lại được.\n",
      "\n",
      "Lỗ đen không thực sự \"ăn\" những thứ xung quanh chúng. Thay vào đó, chúng hấp thụ mọi thứ mà chúng có thể hấp thụ - bất cứ điều gì di chuyển quá gần. Chúng giống như một \"khe hở\" trong vũ trụ, hút mọi thứ vào đó!\n",
      "\n",
      "Nhưng đừng lo lắng, các lỗ đen không ở gần Trái Đất. Chúng nằm ở rất xa và không gây nguy hiểm cho chúng ta.\n"
     ]
    }
   ],
   "source": [
    "# 3. Nội dung hội thoại\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Bạn là một trợ lý AI thông minh và nhiệt tình.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Giải thích về lỗ đen vũ trụ bằng tiếng Việt một cách dễ hiểu cho trẻ em.\"}\n",
    "]\n",
    "\n",
    "# 4. Thực thi và in kết quả\n",
    "outputs = pipe(messages, max_new_tokens=512*4, do_sample=True, temperature=0.9)\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d2c7b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5d0c893faa4834ad6b348e2b16f729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Passing `generation_config` together with generation-related arguments=({'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Hello, how are you?'}, {'role': 'assistant', 'content': \"Hello! I'm an AI assistant created by Alibaba Cloud, so I don't have personal feelings or experiences. I'm here to help you with any questions or information you might need. How can I assist you today?\"}]\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q -U transformers accelerate\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# 1. Load Model without quantization config\n",
    "# We use torch.float16 (Standard Half-Precision) instead of 4-bit\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,  # Standard FP16. Use torch.bfloat16 if on Ampere (A100)\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. Create chat pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example usage\n",
    "messages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "output = pipe(messages, max_new_tokens=128)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ee48cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea948970294b4391933df0566d7b218b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2a72f119e14709b27f392d32cbf6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a991443a468e476baa36bb327216a3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b22c50423c49ba9d8d646a8a8bdacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6780d115cdc41fdbe75841ae4fd8270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92354ede56c44e938e3f396f89b3b371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2208b3fc4e46bcb8e40686fcb0d4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7231eaf385b4b6e86ef0e4311af000e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f8c382a49b4aec8c47c6efbc3b8ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Passing `generation_config` together with generation-related arguments=({'max_new_tokens', 'temperature', 'do_sample'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Answer ---\n",
      "Quantum physics is the branch of physics that deals with phenomena at microscopic scales, such as the behavior of subatomic particles, where the principles of classical mechanics break down and probabilities play a fundamental role in the description of physical properties.\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q -U transformers accelerate bitsandbytes \n",
    "\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "\n",
    "# --- FIX 1: Login explicitly to stop the HF_TOKEN warning ---\n",
    "# Replace 'hf_...' with your actual token from https://huggingface.co/settings/tokens\n",
    "# If you don't have one or don't want to add it, you can delete this line (Qwen is public).\n",
    "# login(token=\"hf_YOUR_TOKEN_HERE\") \n",
    "\n",
    "# Suppress minor warnings to keep output clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# --- FIX 2: Force Float16 to stop the MatMul cast warning ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,  # Force standard FP16 to match 8-bit requirements\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9ff5e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=5120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Answer ---\n",
      "Quantum physics, also known as quantum mechanics, is a fundamental theory in physics that describes the physical properties of nature at the scale of atoms and subatomic particles (such as electrons, protons, and photons). It provides a mathematical description of much of the dual particle-like and wave-like behavior and interactions of energy and matter.\n",
      "\n",
      "### Key Concepts:\n",
      "\n",
      "1. **Wave-Particle Duality**:\n",
      "   - Quantum objects exhibit both particle-like and wave-like behavior. For example, light can be described as either photons (particles) or electromagnetic waves, depending on the experiment.\n",
      "   \n",
      "2. **Quantization**:\n",
      "   - Energy is not continuous but comes in discrete packets called quanta. This concept was introduced by Max Planck to explain blackbody radiation.\n",
      "\n",
      "3. **Superposition**:\n",
      "   - Quantum systems can exist in multiple states simultaneously until they are measured. For instance, an electron in an atom can be in a superposition of different energy levels until it is observed.\n",
      "\n",
      "4. **Uncertainty Principle**:\n",
      "   - Formulated by Werner Heisenberg, this principle states that it is impossible to simultaneously know both the exact position and the exact momentum of a particle. The more precisely one property is measured, the less precisely the other can be controlled or known.\n",
      "\n",
      "5. **Entanglement**:\n",
      "   - Particles can become entangled such that the state of one particle is directly related to the state of another, no matter the distance between them. This phenomenon has been confirmed through numerous experiments.\n",
      "\n",
      "6. **Wave Function**:\n",
      "   - A mathematical function used to describe the quantum state of a system. The square of the absolute value of the wave function gives the probability density of finding a particle in a given state.\n",
      "\n",
      "7. **Quantum Tunneling**:\n",
      "   - Particles can pass through potential barriers that they classically shouldn't be able to pass through. This effect is crucial for many technologies like scanning tunneling microscopes and semiconductor devices.\n",
      "\n",
      "8. **Quantum Mechanics vs. Classical Mechanics**:\n",
      "   - While classical mechanics works well for macroscopic objects, quantum mechanics is necessary to understand phenomena at the microscopic level. The two theories are consistent with each other but describe different regimes of nature.\n",
      "\n",
      "### Applications:\n",
      "\n",
      "- **Quantum Computing**: Uses quantum bits (qubits) that can exist in multiple states simultaneously, allowing for vastly more powerful computations than classical computers for certain tasks.\n",
      "- **Quantum Cryptography**: Utilizes the principles of quantum mechanics to create secure communication channels.\n",
      "- **Semiconductors and Electronics**: Modern electronics rely heavily on quantum mechanical effects in materials.\n",
      "- **Nuclear Physics**: Quantum mechanics is essential for understanding nuclear reactions and the structure of atomic nuclei.\n",
      "\n",
      "### Interpretations:\n",
      "\n",
      "- **Copenhagen Interpretation**: One of the most widely accepted interpretations, which posits that the wave function collapses upon measurement, leading to a single definite outcome.\n",
      "- **Many-Worlds Interpretation**: Proposes that every possible outcome of a quantum measurement actually occurs in some \"world\" or branch of the universe.\n",
      "- **De Broglie–Bohm Theory**: Also known as pilot-wave theory, suggests that particles have definite positions and follow trajectories guided by a pilot wave.\n",
      "\n",
      "Quantum physics is a rich and complex field that continues to challenge our understanding of reality and opens up new possibilities in technology and science.\n"
     ]
    }
   ],
   "source": [
    "# --- FIX 3: Clean Pipeline Call ---\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Explain quantum physics \"}]\n",
    "\n",
    "# We use return_full_text=False so you only get the answer, not the prompt repeated\n",
    "output = pipe(\n",
    "    messages, \n",
    "    max_new_tokens=512*10, \n",
    "    do_sample=True, \n",
    "    temperature=0.7,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "print(\"\\n--- Model Answer ---\")\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
