import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, urlunparse
import re
import time

def get_manual_map(url):
    """
    T√¨m v√† tr·∫£ v·ªÅ danh s√°ch c√°c URL t·ª´ sitemap.xml ho·∫∑c b·∫±ng c√°ch crawl trang web
    
    Args:
        url (str): URL c·ªßa website c·∫ßn map
    
    Returns:
        list: Danh s√°ch c√°c URL t√¨m th·∫•y
    """
    print(f"üîç ƒêang t√¨m Map th·ªß c√¥ng cho: {url}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    links_found = set()
    domain = urlparse(url).netloc
    
    # Chu·∫©n h√≥a URL (th√™m schema n·∫øu thi·∫øu)
    if not urlparse(url).scheme:
        url = 'https://' + url
    
    # --- C√ÅCH 1: Th·ª≠ t√¨m trong c√°c file sitemap (Chu·∫©n SEO) ---
    sitemap_paths = [
        "/sitemap.xml",
        "/sitemap_index.xml",
        "/sitemap-index.xml",
        "/sitemap.php",
        "/sitemap.txt",
        "/sitemap.xml.gz"
    ]
    
    for path in sitemap_paths:
        sitemap_url = urljoin(url, path)
        try:
            print(f"  ‚Ü≥ ƒêang th·ª≠: {sitemap_url}")
            response = requests.get(sitemap_url, headers=headers, timeout=15, allow_redirects=True)
            
            if response.status_code == 200:
                content_type = response.headers.get('Content-Type', '').lower()
                
                if 'xml' in content_type or 'text/xml' in content_type:
                    print(f"‚úÖ T√¨m th·∫•y file sitemap: {sitemap_url}")
                    
                    # Parse sitemap XML
                    soup = BeautifulSoup(response.content, 'lxml-xml')
                    
                    # T√¨m t·∫•t c·∫£ URL trong sitemap (x·ª≠ l√Ω c·∫£ sitemap index)
                    urls = []
                    
                    # T√¨m trong <loc> tags
                    loc_tags = soup.find_all('loc')
                    if loc_tags:
                        urls = [loc.text.strip() for loc in loc_tags]
                    else:
                        # Th·ª≠ t√¨m trong <url> tags
                        url_tags = soup.find_all('url')
                        for url_tag in url_tags:
                            loc = url_tag.find('loc')
                            if loc:
                                urls.append(loc.text.strip())
                    
                    # L·ªçc c√°c URL c√πng domain
                    for found_url in urls:
                        parsed_url = urlparse(found_url)
                        if parsed_url.netloc == domain or not parsed_url.netloc:
                            # Chu·∫©n h√≥a URL
                            normalized_url = urlunparse(parsed_url._replace(
                                fragment='',  # B·ªè fragment (#)
                                query=''      # C√≥ th·ªÉ gi·ªØ query n·∫øu mu·ªën, ho·∫∑c b·ªè
                            ))
                            links_found.add(normalized_url)
                    
                    if links_found:
                        print(f"  ‚Ü≥ T√¨m th·∫•y {len(links_found)} URL t·ª´ sitemap")
                        return sorted(list(links_found))
                        
        except requests.exceptions.RequestException as e:
            continue
        except Exception as e:
            print(f"  ‚Ü≥ L·ªói khi x·ª≠ l√Ω {sitemap_url}: {str(e)[:50]}...")
            continue
    
    # --- C√ÅCH 2: C√†o tr·ª±c ti·∫øp c√°c link t·ª´ trang ch·ªß ---
    print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y sitemap, ƒëang qu√©t link tr·ª±c ti·∫øp t·ª´ trang ch·ªß...")
    
    try:
        response = requests.get(url, headers=headers, timeout=15, allow_redirects=True)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # T√¨m t·∫•t c·∫£ c√°c th·∫ª c√≥ href
        for tag in soup.find_all(['a', 'link'], href=True):
            href = tag['href'].strip()
            
            # B·ªè qua c√°c link kh√¥ng c·∫ßn thi·∫øt
            if href.startswith(('javascript:', 'mailto:', 'tel:', '#', 'data:')):
                continue
            
            # Chuy·ªÉn link t∆∞∆°ng ƒë·ªëi th√†nh link tuy·ªát ƒë·ªëi
            full_url = urljoin(url, href)
            
            # Parse v√† chu·∫©n h√≥a URL
            parsed = urlparse(full_url)
            
            # Ch·ªâ l·∫•y c√°c link c√πng domain
            if parsed.netloc == domain:
                # Chu·∫©n h√≥a URL
                normalized_url = urlunparse(parsed._replace(
                    fragment='',  # B·ªè fragment
                    path=parsed.path.rstrip('/') or '/'  # Chu·∫©n h√≥a path
                ))
                links_found.add(normalized_url)
        
        # Th·ª≠ t√¨m trong robots.txt
        robots_url = urljoin(url, "/robots.txt")
        try:
            robots_response = requests.get(robots_url, headers=headers, timeout=10)
            if robots_response.status_code == 200:
                # T√¨m Sitemap directive trong robots.txt
                for line in robots_response.text.split('\n'):
                    if line.lower().startswith('sitemap:'):
                        sitemap_url = line.split(':', 1)[1].strip()
                        print(f"  ‚Ü≥ T√¨m th·∫•y sitemap trong robots.txt: {sitemap_url}")
                        # C√≥ th·ªÉ th√™m logic ƒë·ªÉ parse sitemap n√†y
        except:
            pass
            
    except requests.exceptions.RequestException as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi: {e}")
    except Exception as e:
        print(f"‚ùå L·ªói khi x·ª≠ l√Ω trang: {e}")
    
    result = sorted(list(links_found))
    print(f"  ‚Ü≥ T·ªïng c·ªông t√¨m th·∫•y {len(result)} URL")
    return result

# --- H√†m b·ªï tr·ª£ ƒë·ªÉ l∆∞u k·∫øt qu·∫£ ---
def save_results_to_file(urls, filename="website_map.txt"):
    """L∆∞u k·∫øt qu·∫£ v√†o file"""
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"Website Map - Generated at {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("="*80 + "\n\n")
        for i, url in enumerate(urls, 1):
            f.write(f"{i:3d}. {url}\n")
    print(f"üíæ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o file: {filename}")

# --- CH·∫†Y TH·ª¨ ---
if __name__ == "__main__":
    # Test v·ªõi m·ªôt s·ªë website
    test_urls = [
        "https://docs.tavily.com",
        # "https://example.com",
        # "google.com"  # C≈©ng x·ª≠ l√Ω ƒë∆∞·ª£c URL kh√¥ng c√≥ scheme
    ]
    
    for target in test_urls:
        print("\n" + "="*80)
        print(f"B·∫Øt ƒë·∫ßu mapping: {target}")
        print("="*80)
        
        try:
            map_results = get_manual_map(target)
            
            if map_results:
                print(f"\n‚úÖ K·∫øt qu·∫£ Map (T√¨m th·∫•y {len(map_results)} link):")
                for i, link in enumerate(map_results[:20], 1):  # Hi·ªÉn th·ªã 20 link ƒë·∫ßu
                    print(f"{i:3d}. {link}")
                
                if len(map_results) > 20:
                    print(f"... v√† {len(map_results) - 20} link n·ªØa")
                
                # L∆∞u to√†n b·ªô k·∫øt qu·∫£ v√†o file
                domain = urlparse(target).netloc or target
                filename = f"{domain.replace('.', '_')}_map.txt"
                save_results_to_file(map_results, filename)
            else:
                print("‚ùå Kh√¥ng t√¨m th·∫•y link n√†o!")
                
        except KeyboardInterrupt:
            print("\n‚èπÔ∏è ƒê√£ d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
            break
        except Exception as e:
            print(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")